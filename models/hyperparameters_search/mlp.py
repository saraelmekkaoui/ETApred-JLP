# -*- coding: utf-8 -*-
"""MLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hk07kIxMXsyAVSX0jZudNnai13XtgkrD
"""

!pip install wandb -qqq
import wandb
# Log in to your W&B account
wandb.login()
import sys
import numpy as np 
import math
import pandas as pd 
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D, Flatten
from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, Callback, EarlyStopping
from wandb.keras import WandbCallback

# Import data
data_path = '/data/data_split/data_nn_seq'
x_train = np.load(data_path + '/x_train.npy')
x_test = np.load(data_path + '/x_test.npy')
x_valid = np.load(data_path + '/x_valid.npy')
y_train = np.load(data_path + '/y_train.npy')
y_test = np.load(data_path + '/y_test.npy')
y_valid = np.load(data_path + '/y_valid.npy')

# Create inputs sets

#0:'Max_Draught'
#1: 'Latitude'
#2: 'Longitude'
#3: 'Speed_over_Ground'
#4: 'COG_cos'
#5: 'COG_sin'
#6: 'TH_cos'
#7: 'TH_sin'
#8: 'Navigational_Status_0.0'
#9: 'Navigational_Status_1.0'
#10: 'Navigational_Status_2.0'
#11: 'Navigational_Status_3.0'
#12: 'Navigational_Status_4.0'
#13: 'Na13vigational_Status_5.0'
#14: 'Nav14igational_Status_8.0'
#15: 'Navi15gational_Status_15.0'
#16: 'GT'
#17: 'DWT'
#18: 'LOA'
#19: 'BEAM'
#20: 'VesselTypeB_Cargo'
#21: 'VesselTypeB_Tanker'
#22: 'Age'
#23: 'current_uo' 
#24: 'current_vo'
#25: 'wind_u10'
#26: 'wind_v10'
#27: 'mwd' 
#28: 'mwp' 
#29: 'swh'
#30: 'sst'  
#31: 'Origin_Lat'
#32: 'Origin_Lon'
#33: 'acc_dist' 
#34: 'acc_time_hours'
#35: 'leg_distance'
#36: 'leg_speed'
#37: 'leg_elapsed_time_hours'
#38: 'remaining_distance'


### Inputs set: All varaibles -- AIS + Vessel particulars + Weather + Crafted features ###
###########################################################################################
#don't remove any variable

### Inputs set: AIS + Vessel particulars ###
###################################################

x_train= np.delete(x_train,[23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38],1)
x_test= np.delete(x_test,[23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38],1)
x_valid= np.delete(x_valid,[23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38],1)

### Inputs set: AIS + Vessel particulars ###
###################################################

x_train= np.delete(x_train,[31,32,33,34,35,36,37,38],1)
x_test= np.delete(x_test,[31,32,33,34,35,36,37,38],1)
x_valid= np.delete(x_valid,[31,32,33,34,35,36,37,38],1)

### Inputs set: AIS + Vessel particulars ###
###################################################

x_train= np.delete(x_train,[23,24,25,26,27,28,29,30],1)
x_test= np.delete(x_test,[23,24,25,26,27,28,29,30],1)
x_valid= np.delete(x_valid,[23,24,25,26,27,28,29,30],1)

print('x_train shape:', x_train.shape)
print('y_train shape:', y_train.shape)
print('x_valid shape:', x_valid.shape)
print('y_valid shape:', y_valid.shape)
print('x_test shape:', x_test.shape)
print('y_test shape:', y_test.shape)

n_features = x_train.shape[1]
print('n_features = ', n_features)

# Configure the sweep 
sweep_config = {
    'method': 'random', 
    'MLP-',
    'description':'Features set: AIS + VesselParticulars + Weather + CraftedFeatures',
    'metric': {
      'name': 'val_loss',
      'goal': 'minimize'   
    },
    'parameters': {
 
         'dropout': {
            'values': [0.0,0.1, 0.2, 0.3, 0.4, 0.5]
        },
 
        'batch_size': {
            'values': [64, 128, 256]
        },

        'log_learning_rate': {
        # a flat distribution between -5 and -1
        'distribution': 'uniform',
        'distribution': 'uniform',
        'min': -5,
        'max': -1
      },
      
        'units_1': {
        # integers between 50 and 700
        # with evenly-distributed logarithms 
        'distribution': 'q_log_uniform',
        'q': 1,
        'min': math.log(50),
        'max': math.log(700)
        }
        ,
      
        'units_2': {
        # integers between 50 and 700
        # with evenly-distributed logarithms 
        'distribution': 'q_log_uniform',
        'q': 1,
        'min': math.log(50),
        'max': math.log(700)
        } 
        ,
      
        'units_3': {
        # integers between 50 and 700
        # with evenly-distributed logarithms 
        'distribution': 'q_log_uniform',
        'q': 1,
        'min': math.log(50),
        'max': math.log(700)
        },
        'units_4': {
        # integers between 50 and 700
        # with evenly-distributed logarithms 
        'distribution': 'q_log_uniform',
        'q': 1,
        'min': math.log(50),
        'max': math.log(700)
        },

        'units_5': {
        # integers between 50 and 700
        # with evenly-distributed logarithms 
        'distribution': 'q_log_uniform',
        'q': 1,
        'min': math.log(50),
        'max': math.log(700)
        }    

    }
}

# Initialize a new sweep
sweep_id = wandb.sweep(sweep_config, entity="your_entity", project="project_name")

# The sweep calls this function with each set of hyperparameters
def train():
    # Default values for hyper-parameters we're going to sweep over
    config_defaults = {
        'epochs': 500,
        'batch_size': 64,
        'log_learning_rate': -2,
        'units_1': 150,
        'units_2': 150,
        'units_3': 150,
        'units_3': 150,
        'units_3': 150,
        'dropout': 0.3,
        'seed': 42
    }

    # Initialize a new wandb run
    wandb.init(config=config_defaults)
    
    # Config is a variable that holds and saves hyperparameters and inputs
    config = wandb.config
    
    # Define the model architecture 

    model = keras.Sequential()
    model.add(keras.layers.InputLayer(input_shape=(n_features,)))
    model.add(layers.Dense(units= config.units_1, activation='relu'))
    model.add(layers.Dropout(config.dropout))
    model.add(layers.Dense(units= config.units_2, activation='relu'))
    model.add(layers.Dropout(config.dropout))
    model.add(layers.Dense(units= config.units_3, activation='relu'))
    model.add(layers.Dropout(config.dropout))
    model.add(layers.Dense(units= config.units_4, activation='relu'))
    model.add(layers.Dropout(config.dropout))
    model.add(layers.Dense(units= config.units_5, activation='relu'))
    model.add(layers.Dropout(config.dropout))
    model.add(layers.Dense(1,activation='relu'))

    model.compile(optimizer=keras.optimizers.Adam(learning_rate = 10**config.log_learning_rate), loss='mse', metrics=['mae'])

    model.fit(x_train, y_train, batch_size=config.batch_size,
              epochs=config.epochs,
              shuffle = False,
              validation_data=(x_valid, y_valid),
              callbacks=[WandbCallback(),
                          EarlyStopping(patience=10, restore_best_weights=True)])
    
    train_loss, train_mae = model.evaluate(x_train, y_train, verbose=2)
    valid_loss, valid_mae = model.evaluate(x_valid, y_valid, verbose=2)
    test_loss, test_mae = model.evaluate(x_test, y_test, verbose=2)
    wandb.log({'train_loss': train_loss, 'train_mae': train_mae,
               'valid_loss': valid_loss, 'valid_mae': valid_mae,
               'test_loss': test_loss, 'test_mae': test_mae})
    
# Run the sweep
wandb.agent(sweep_id, train, count = 40)