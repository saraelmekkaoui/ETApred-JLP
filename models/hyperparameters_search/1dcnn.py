# -*- coding: utf-8 -*-
"""1DCNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IR4AwiHqFadoYUDDf16nn0xH3Umqe1Lb
"""

!pip install wandb -qqq
import wandb
# Log in to your W&B account
wandb.login()
import sys
import numpy as np 
import math
import pandas as pd 
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Model
from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D, Flatten
from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, Callback, EarlyStopping
from cond_rnn import ConditionalRecurrent
from wandb.keras import WandbCallback

# Import data
data_path = '/data/data_split/data_seq'
x_train = np.load(data_path + '/x_train.npy')
x_test = np.load(data_path + '/x_test.npy')
x_valid = np.load(data_path + '/x_valid.npy')
y_train = np.load(data_path + '/y_train.npy')
y_test = np.load(data_path + '/y_test.npy')
y_valid = np.load(data_path + '/y_valid.npy')

# Create inputs sets

#0:'Max_Draught'
#1: 'Latitude'
#2: 'Longitude'
#3: 'Speed_over_Ground'
#4: 'COG_cos'
#5: 'COG_sin'
#6: 'TH_cos'
#7: 'TH_sin'
#8: 'Navigational_Status_0.0'
#9: 'Navigational_Status_1.0'
#10: 'Navigational_Status_2.0'
#11: 'Navigational_Status_3.0'
#12: 'Navigational_Status_4.0'
#13: 'Na13vigational_Status_5.0'
#14: 'Nav14igational_Status_8.0'
#15: 'Navi15gational_Status_15.0'
#16: 'GT'
#17: 'DWT'
#18: 'LOA'
#19: 'BEAM'
#20: 'VesselTypeB_Cargo'
#21: 'VesselTypeB_Tanker'
#22: 'Age'
#23: 'current_utotal' 
#24: 'current_vtotal'
#25: 'wind_u10'
#26: 'wind_v10'
#27: 'mwd' 
#28: 'mwp' 
#29: 'swh'
#30: 'sst'  
#31: 'Origin_Lat'
#32: 'Origin_Lon'
#33: 'acc_dist' 
#34: 'acc_time_hours'
#35: 'leg_distance'
#36: 'leg_speed'
#37: 'leg_elapsed_time_hours'
#38: 'remaining_distance'


# Data split of sequence and non sequence variables

### Inputs set: All varaibles -- AIS + Vessel particulars + Weather + Crafted features ###
###########################################################################################
#sequence variables
x_train_nn_cc= np.delete(x_train,[16,17,18,19,20,21,22,31,32],2)
x_test_nn_cc= np.delete(x_test,[16,17,18,19,20,21,22,31,32],2)
x_valid_nn_cc= np.delete(x_valid,[16,17,18,19,20,21,22,31,32],2)

#non sequence variables 
x_train_cc_split= np.delete(x_train,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38],2)
x_test_cc_split= np.delete(x_test,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38],2)
x_valid_cc_split= np.delete(x_valid,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38],2)

### Inputs set: AIS + Vessel particulars ###
###################################################
#sequence variables

#x_train_nn_cc= np.delete(x_train,[16,17,18,19,20,21,22,31,32,23,24,25,26,27,28,29,30,33,34,35,36,38],2)
#x_test_nn_cc= np.delete(x_test,[16,17,18,19,20,21,22,31,32,23,24,25,26,27,28,29,30,33,34,35,36,38],2)
#x_valid_nn_cc= np.delete(x_valid,[16,17,18,19,20,21,22,31,32,23,24,25,26,27,28,29,30,33,34,35,36,38],2)

#non sequence variables 

#x_train_cc_split= np.delete(x_train,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38,31,32],2)
#x_test_cc_split= np.delete(x_test,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38,31,32],2)
#x_valid_cc_split= np.delete(x_valid,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38,31,32],2)

### Inputs set: AIS + Vessel particulars ###
###################################################
#sequence variables

#x_train_nn_cc= np.delete(x_train,[16,17,18,19,20,21,22,31,32,33,34,35,36,38],2)
#x_test_nn_cc= np.delete(x_test,[16,17,18,19,20,21,22,31,32,33,34,35,36,38],2)
#x_valid_nn_cc= np.delete(x_valid,[16,17,18,19,20,21,22,31,32,33,34,35,36,38],2)

#non sequence variables 
#x_train_cc_split= np.delete(x_train,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38,31,32],2)
#x_test_cc_split= np.delete(x_test,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38,31,32],2)
#x_valid_cc_split= np.delete(x_valid,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38,31,32],2)

### Inputs set: AIS + Vessel particulars ###
###################################################
#sequence variables

#x_train_nn_cc= np.delete(x_train,[16,17,18,19,20,21,22,31,32,23,24,25,26,27,28,29,30],2)
#x_test_nn_cc= np.delete(x_test,[16,17,18,19,20,21,22,31,32,23,24,25,26,27,28,29,30],2)
#x_valid_nn_cc= np.delete(x_valid,[16,17,18,19,20,21,22,31,32,23,24,25,26,27,28,29,30],2)

#non sequence variables 

#x_train_cc_split= np.delete(x_train,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38],2)
#x_test_cc_split= np.delete(x_test,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38],2)
#x_valid_cc_split= np.delete(x_valid,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38],2)


print('x_train shape:', x_train.shape)
print('y_train shape:', y_train.shape)
print('x_valid shape:', x_valid.shape)
print('y_valid shape:', y_valid.shape)
print('x_test shape:', x_test.shape)
print('y_test shape:', y_test.shape)
print('')
print('x_train_nn_cc shape:', x_train_nn_cc.shape)
print('x_valid_nn_cc shape:', x_valid_nn_cc.shape)
print('x_test_nn_cc shape:', x_test_nn_cc.shape)
print('')
print('x_train_cc shape:', x_train_cc_split.shape)
print('x_valid_cc shape:', x_valid_cc_split.shape)
print('x_test_cc shape:', x_test_cc_split.shape)

n_features_nn_cc = x_train_nn_cc.shape[2]
n_features_cc = x_train_cc_split.shape[2]
steps = x_train.shape[1]
print('n_features_nn_cc = ', n_features_nn_cc)
print('n_features_cc = ', n_features_cc)
print('steps = ', steps)

#get non sequence data into the right shape
def get_cc_data(data):
  X = []
  for i in range(data.shape[0]):
    x = data[i,0,:]
    x = x.reshape (1,9) 
    X.append(x)
  X = np.array(X).reshape(data.shape[0],data.shape[2])
  return X  

x_train_cc = get_cc_data(x_train_cc_split)
x_valid_cc = get_cc_data(x_valid_cc_split)
x_test_cc = get_cc_data(x_test_cc_split)

# Configure the sweep 
sweep_config = {
    'method': 'random', 
    'name':'CNN-0',
    'metric': {
      'name': 'val_loss',
      'goal': 'minimize'   
    },
    'parameters': {
 
        'log_l2': {
        # a flat distribution between -4 and -1
        'distribution': 'uniform',
        'min': -4,
        'max': -1
      },

        'batch_size': {
            'values': [64, 128, 256]
        },

        'filters_1': {
        # integers between 32 and 64
        # with evenly-distributed logarithms 
        'distribution': 'q_log_uniform',
        'q': 1,
        'min': math.log(32),
        'max': math.log(64)
        },

        'filters_2': {
        # integers between 64 and 96
        # with evenly-distributed logarithms 
        'distribution': 'q_log_uniform',
        'q': 1,
        'min': math.log(64),
        'max': math.log(96)
        },

        'filters_3': {
        # integers between 96 and 128
        # with evenly-distributed logarithms 
        'distribution': 'q_log_uniform',
        'q': 1,
        'min': math.log(96),
        'max': math.log(128)
        },

        'filters_4': {
        # integers between 128 and 160
        # with evenly-distributed logarithms 
        'distribution': 'q_log_uniform',
        'q': 1,
        'min': math.log(128),
        'max': math.log(160)
        },

        'filters_5': {
        # integers between 160 and 192
        # with evenly-distributed logarithms 
        'distribution': 'q_log_uniform',
        'q': 1,
        'min': math.log(160),
        'max': math.log(192)
        },

        'kernels': {
        # integers between 2 and 3
        # with discrete uniform distribution
        'distribution': 'int_uniform',
        'min': 2,
        'max': 3
        },

        'log_learning_rate': {
        # a flat distribution between -5 and -1
        'distribution': 'uniform',
        'min': -5,
        'max': -1
        }
    }
}

# Initialize a new sweep
sweep_id = wandb.sweep(sweep_config, entity="your_entity", project="project_name")

# The sweep calls this function with each set of hyperparameters
def train():
    # Default values for hyper-parameters we're going to sweep over
    config_defaults = {
        'epochs': 500,
        'batch_size': 64,
        'log_learning_rate': -2,
        'log_l2':-2,
        'filters_1': 32,
        'filters_2': 64,
        'filters_3': 96,
        'filters_4': 128,
        'filters_5': 160,
        'kernels': 2,
        'seed': 42
    }

    # Initialize a new wandb run
    wandb.init(config=config_defaults)
    
    # Config is a variable that holds and saves hyperparameters and inputs
    config = wandb.config
    
    # Define the model architecture 

    i = Input(shape=(steps, n_features_nn_cc))
    ic = Input(shape=(n_features_cc))

    k = Dense(10, activation = 'relu')(ic) 

    m = Conv1D(filters= config.filters_1, kernel_size = config.kernels, padding='causal',dilation_rate= 1, activation='relu', 
               kernel_regularizer=tf.keras.regularizers.L2(l2=10**config.log_l2))(i)
    m = Conv1D(filters= config.filters_2, kernel_size = config.kernels, padding='causal',dilation_rate= 2, activation='relu',
               kernel_regularizer=tf.keras.regularizers.L2(l2=10**config.log_l2))(m)
    m = Conv1D(filters= config.filters_3, kernel_size = config.kernels, padding='causal',dilation_rate= 4, activation='relu',
               kernel_regularizer=tf.keras.regularizers.L2(l2=10**config.log_l2))(m)
    m = Conv1D(filters= config.filters_4, kernel_size = config.kernels, padding='causal',dilation_rate= 8, activation='relu',
               kernel_regularizer=tf.keras.regularizers.L2(l2=10**config.log_l2))(m)
    m = Conv1D(filters= config.filters_5, kernel_size = config.kernels, padding='causal',dilation_rate= 16, activation='relu',
               kernel_regularizer=tf.keras.regularizers.L2(l2=10**config.log_l2))(m)

    kRepeated = RepeatVector(steps)(k)
    out = Concatenate(axis=2)([m, kRepeated])
    m = Dense(1, activation = 'relu')(out) 

    model = Model([i, ic], m) 


    model.compile(optimizer=keras.optimizers.Adam(learning_rate = 10**config.log_learning_rate), loss='mse', metrics=['mae'])

    model.fit([x_train_nn_cc, x_train_cc], y_train, batch_size=config.batch_size,
              epochs=config.epochs,
              shuffle = False,
              validation_data=([x_valid_nn_cc, x_valid_cc], y_valid),
              callbacks=[WandbCallback(),
                          EarlyStopping(patience=10, restore_best_weights=True)])
    
    train_loss, train_mae = model.evaluate([x_train_nn_cc, x_train_cc], y_train, verbose=2)
    valid_loss, valid_mae = model.evaluate([x_valid_nn_cc, x_valid_cc], y_valid, verbose=2)
    test_loss, test_mae = model.evaluate([x_test_nn_cc, x_test_cc], y_test, verbose=2)
    wandb.log({'train_loss': train_loss, 'train_mae': train_mae,
               'valid_loss': valid_loss, 'valid_mae': valid_mae,
               'test_loss': test_loss, 'test_mae': test_mae})
# Run the sweep
wandb.agent(sweep_id, train,count = 40)